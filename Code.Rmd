---
title: "<center> **Analysis and classification of iris species** </center>"
author: "<center> **Krzysztof Dyba** </center>"
date: "<center> `r Sys.Date()` </center>"
output: 
  html_document:
    toc: true
    toc_float: true
    toc_depth: 2
    theme: spacelab
    highlight: tango
---

<style>
body {text-align: justify}
</style>

```{r include = FALSE}
startTime = Sys.time()
```

## **Load libraries and data**

**Load libraries**

```{r message = FALSE, warning = FALSE}
library("MASS")
library("e1071")
library("tidyr")
library("caret")
library("GGally")
library("ggplot2")
library("doParallel")
```

```{r}
# save session info
writeLines(capture.output(sessionInfo()), "sessionInfo.txt")
```

**Load data**

```{r}
data = read.csv("data/C4L_Academy-Data_Science_Graduate-IRISES_dataset_(2019-06).csv", sep = "|")
data$Petal.Width[133] = "2.2"
data$Petal.Width = as.numeric(as.character(data$Petal.Width))
```

In the 133rd row of the *Petal.Width* column, a comma was used instead of a dot as a decimal separator. This error causes the data type to be incorrectly read as text, and when converted to numeric form, the missing data (*NA*) is returned in that cell. The separator was changed to a dot and then column type to number.

## **Data exploration** {.tabset .tabset-fade}

### Description

```{r}
str(data)
```

The dataset consists of 150 observations and 5 variables (four numeric and one categorizing). Numeric variables determine the length and width of the flower sepal and the length and width of the flower petal. The values are expressed in centimeters.

### Categories

```{r collapse = TRUE}
levels(data$Species)
table(data$Species)
```

The categorizing variable consists of three species of irises, i.e. **setosa**, **versicolor** and **virginica**. Each type of iris contains the same number of observations, so there is no problem of class imbalance.

### Missing data

```{r collapse = TRUE}
apply(data[1:4], 2, function(x) any(is.na(x)))
sum(is.na(data$Sepal.Width))
which(is.na(data$Sepal.Width))
data$Sepal.Width[82] = round(mean(data$Sepal.Width[data$Species == "versicolor"], na.rm = TRUE), 1)
```

Only the *Sepal.Width* column had one missing value (NA). The measurement was omitted for iris **versicolor**. The average value of the sepal width for this species was calculated (i.e. 2.78 cm), and then the missing measurement was completed.

### Outliers

```{r fig.height = 3.5}
data_long = gather(data, "variable", "value", 1:4)
ggplot(data_long, aes(x = variable, y = value)) + 
  geom_boxplot(aes(colour = Species)) +
  xlab("Variable") +
  ylab("Value [cm]") +
  labs(colour = "Species") +
  theme_light()
```

For the *Sepal.Length* variable, one negative value of -4.8 cm is noticeable. This is obviously an invalid value. Probably the minus sign was written by mistake.

```{r}
data$Sepal.Length[data$Sepal.Length == -4.8] = abs(-4.8)
data_long$value[data_long$value == -4.8] = abs(-4.8)
```

The error was corrected by changing to a positive value.
No other outliers were noticed that would result from an incorrect entry.

### Duplicates
```{r}
sum(duplicated(data[1:4]))
```

There is one duplicate row with the measurement values (species **virginica**) in the dataset. However, this is not an anomaly.

## **Data analysis** 

```{r message = FALSE, fig.width = 9}
ggpairs(data, mapping = aes(color = Species),
        diag = list(continuous = wrap("densityDiag", alpha = 0.8)),
        lower = list(continuous = wrap("points", alpha = 0.7)),
        columns = c("Sepal.Length", "Sepal.Width", "Petal.Length", "Petal.Width")) +
  theme_light()
```

### {.tabset .tabset-fade}

#### Statistics

```{r}
aggregate(. ~ Species, data = data, mean)
```

Average values of explanatory variables for particular iris species are presented above. Species **setosa** is average the smallest in terms of the length and width of flower petals, while the **virginica** species is characterized by their largest average size.

#### Correlation
Species **setosa** has the largest correlation of 0.74 between the length and width of the flower sepal. For other variables, the correlation is less than 0.33. <br>
Species **versicolor** has the largest correlation between the length and width of flower petal of 0.79. A smaller correlation of 0.75 was noted for petal and sepal lengths. Other variables are moderately correlated. <br>
Species **virginica** has the largest relationship between petal length and sepal length of 0.86. The lowest correlation can be seen in the case of petal width and sepal length.

#### Distribution
In most cases, the distribution of variables is not close to the normal distribution. The exceptions are the length and width of the petals of species iris **versicolor** and **virginica**.

#### Classification

```{r fig.height = 3.5}
ggplot(data_long, aes(value, variable)) +
  geom_jitter(aes(colour = Species), width = 0, alpha = 0.8) +
  xlab("Value [cm]") +
  ylab("Variable") +
  labs(colour = "Species") +
  theme_light()
```

Analyzing the two above figures, you can easily separate the **setosa** species based on the two variables *Petal.Width* and *Petal.Length*. A bigger problem may arise in the classification of **versicolor** and **virginica** species, because the objects of these two species overlap in some cases. Probably in this situation, the variables *Petal.Width* and *Petal.Length* will also play the most important role.

## **Machine learning** {.tabset .tabset-fade}

### Dataset splitting

```{r}
# defined random seed for repeatability of results
set.seed(1)

trainIndex = createDataPartition(data$Species, p = 0.8, list = FALSE)
train = data[trainIndex, ]
test = data[-trainIndex, ]
```

The input dataset was split evenly, taking into account iris species, into a training (120 observations) and a test (30 observations) sets.

### Models training

```{r} 
set.seed(1)
seeds = vector(mode = "list", length = 101) # length = number * repeats + 1
for(i in 1:101) seeds[[i]] = sample.int(n = 1000, 6)

# 10x crossvalidation 10-fold
fitControl = trainControl(method = "repeatedcv", 
                          number = 10, 
                          repeats = 10, 
                          seeds = seeds)

# parallel training
n_cores = detectCores() - 1
cl = makePSOCKcluster(n_cores)
registerDoParallel(cl)

# models
SVM_mdl = train(Species ~ ., data = train, 
                method = "svmLinear2", 
                trControl = fitControl,
                metric = "Accuracy")

RF_mdl = train(Species ~ ., data = train,
               method = "ranger",
               trControl = fitControl,
               metric = "Accuracy")

LDA_mdl = train(Species ~ ., data = train,
                method = "lda2",
                trControl = fitControl,
                metric = "Accuracy")

stopCluster(cl)
```

The three most popular algorithms with proven high efficiency in the scientific literature were selected, i.e. support vector machine (**SVM**), random forests (**RF**) and linear discriminant analysis (**LDA**). <br>

Repeated 10-fold crossvalidation was used for resampling, where the training set is also split into training (9 folds) and test (1 fold) subsets for all combinations and the process is repeated n times. With this procedure overfitting can be avoided, i.e. a situation where the model has learned training data and not the general trend in the data. <br>

The measure of effectiveness was accuracy defined as the quotient of the sum of the truly positive and truly negative classifications to all possibilities. Due to the fact that the training and test sets are balanced, there are no premises for ineffectiveness (bias) of this measure. <br>

In order to reduce the calculation time, the default optimization parameters in the grid were used. However, to obtain optimal models, the space for parameter optimization should be expanded, which will affect the time it takes to get results. <br>

Data splitting and resampling are random operations, so each execution will return different results. To ensure repeatability of results and the ability to reproduce the analysis, fixed random seeds were set. <br>

Calculations were parallelized using 3 processor threads.

### Models comparison

```{r fig.height = 3.5}
resamps = resamples(list(SVM = SVM_mdl, RF = RF_mdl, LDA = LDA_mdl))
ggplot(resamps, metric = "Accuracy", conf.level = 0.99) +
  xlab("Model") +
  ylab("Accuracy") +
  labs(title = "Classification efficiency", subtitle = "Confidence interval: 0,99") +
  theme_light()
```

The model based on linear discriminatory analysis (**LDA**) with a confidence interval of 0.99 proved to be the most effective (with the highest accuracy value). In the resampling process the final accuracy value was over 0.98.

```{r}
# save best model
saveRDS(LDA_mdl, "model_LDA.rds")
```

### Validation

```{r}
pred = predict(LDA_mdl, test)
confusionMatrix(pred, test$Species)
```

Then the validation was carried out based on the previously determined test (independent) set. Test results confirmed the very high efficiency of the **LDA** of approximately 0.97. The model was only mistaken once in 30 cases (**virginica** instead of **versicolor**). However, already at the data analysis stage, difficulties in classifying these two species due to similar species characteristics were foreseen.

### Variable importance

```{r fig.height = 3.5}
imps = data.frame(varImp(LDA_mdl)$importance)
imps$variable = rownames(imps)
imps = gather(imps, "species", "importance", 1:3)

ggplot(imps, aes(x = reorder(variable, importance), y = importance)) +
  geom_col(width = 0.01, fill = "black") +
  geom_point() +
  coord_flip() +
  facet_wrap(. ~ species) +
  xlab("Variable") +
  ylab("Importance") +
  labs(subtitle = "Linear Discriminant Analysis") +
  theme_light()
```

In the case of the **LDA** model, previous assumptions have been confirmed that the *Petal.Width* and *Petal.Length* variables will be most helpful in distinguishing species. The least important variable was *Sepal.Width* for the **virginica** species.

## **Summary**

Data exploration revealed that the input dataset contained some abnormalities (e.g. negative value, missing value or wrong data type), that were corrected. The equivalence of individual species of irises has made it possible to reliably assess the effectiveness of classification. <br>

The analysis of the data allowed to discover the correlation between certain features of the studied iris species and to determine which variables could potentially be most useful for the model development. <br>

The three most popular **SVM**, **RF** and **LDA** models were tested. The **LDA** model provided the best results, i.e. for the training set it obtained over **98%** of correct classifications and for the test set about **97%**. The achieved results confirmed the possibility of using machine learning methods for effective and accurate classification of species of irises. The developed and positively verified model can be used for commercial purposes. <br>

The results of this study are fully repeatable due to the use of the same seed of randomness.

## **Further possibilities**

Despite obtaining a highly effective classifier, there are prospects for improving. Nevertheless, there may be a situation in which the classification will not improve, but only the calculation time and code complexity will increase.

Suggested next steps:

+ dimensional reduction through *Principal Component Analysis* and data transformation (normalization and Box-Cox transformation), 
+ use of the *gradient boosting* type algorithm, e.g. **xgboost**,
+ increase the space for optimization hyperparameters for models **SVM** and **RF**.

---

<center> <font size = "2"> <i>
The code was executed in `r round(difftime(Sys.time(), startTime, units = "secs"))` s.
<center> </font> </i>